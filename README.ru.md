# Enterprise RAG Challenge с годовыми отчётами — Раунд 1
Сентябрь 2024 — [Trustbit](https://www.trustbit.tech) (теперь часть TIMETOACT GROUP как [TIMETOACT Austria](https://www.timetoact-group.at))

Copyright 2024 TIMETOACT Austria, распространяется по лицензии Apache 2.0


## Инструкции
**Это дружеское соревнование для проверки точности разных RAG-систем в бизнес-нагрузках.**

Мы начинаем с задачи вопросов и ответов.

Участники строят систему, способную отвечать на вопросы по загруженным PDF-документам (годовым отчётам). Или они могут протестировать уже готовую AI-ассистент систему.

В соревновании может участвовать любой. Возможна анонимная подача. Мы просим участников поделиться некоторыми деталями об их реализации RAG — это полезно для сообщества. Мы хотим понять, что работает лучше на практике, и поделиться результатами.

Когда наступит момент соревнования:

1. Участникам заранее выдаётся набор годовых отчётов в формате PDF. У них будет время на обработку этих файлов.
2. Генерируется список вопросов для этих файлов. В течение нескольких минут (чтобы избежать ручной доработки) участникам необходимо будет предоставить ответы и загрузить их.
3. После этого ответы проверяются публично и компилируются в общий публичный датасет.

Все ответы и данные будут собраны в публичный набор данных. Вы сможете сравнить результаты разных команд и технологий (если команда ответила на часть вопросов) в одной таблице. В конце конкурса мы также подготовим и опубликуем отчёт.


*Часто задаваемые вопросы*

В: Поделитесь ли вы всеми 46 ГБ годовых отчётов?

О: Нет, мы опубликуем только подмножество для соревнования. Эти отчёты являются публичными; при желании вы можете собрать их сами из открытых источников.

В: Почему вопросы содержат названия компаний, которых нет в PDF?

О: Цель — выявить галлюцинации. Если данные о компании отсутствуют в файлах соревнования, RAG-система должна ответить `N/A`.


В: Почему некоторые вопросы не имеют смысла?

О: Вы увидите вопросы, которые выглядят корректными на первый взгляд, но для конкретной компании бессмысленны. Например:

* number: How many stores did "Strike Energy Limited" have in the end of fiscal year 2021?

Как и в реальном мире — не на все вопросы есть релевантная информация. Если у компании нет магазинов, ответ — `N/A`.

Плохая RAG-система выдаст правдоподобную, но неверную цифру. Хорошая — корректно ответит `N/A`.


В: Как проверяются правильные ответы?

О: Trustbit собирает все ответы сначала. Затем мы вручную проверим вопросы и составим эталонные ответы. Правильные ответы будут опубликованы вместе с оценёнными ответами.


В: Хочу провести похожий челлендж с таблицами, другим языком или индустрией. Как это сделать?

О: Репозиторий доступен по лицензии Apache 2.0 — его можно использовать без ограничений. Вы можете форкнуть репозиторий, затем:

1. Собрать собственный набор исходных файлов.
2. Настроить генератор вопросов под вашу предметную область.
3. Следовать общей процедуре.

Просим только соблюдать условия лицензии Apache 2.0.


## Вопросы
Раунд 1 был тестовым прогонами, чтобы отшлифовать процесс соревнования.

- Выбор PDF начался 15 августа в 10:00 CET. См. папку `pdfs`.
- Генерация вопросов началась в 12:00 CET. См. `questions.json`.

Выходы генерации seed приведены ниже (см. тег `round01`, если хотите воспроизвести):

Seed 1 (UTC время):

```text
856853 at 2024-08-15 08:05:07 (...eaa76b09)
# Deterministic seed: 3936840457
```

Выход генерации Seed 2 (UTC время):

```text
# New block found! 856868 at 2024-08-15 10:05:01 (...2720eb96)
# Deterministic seed: 656468886
```


## Ответы

Отправки ответов доступны в папке `submissions`.

«Правильные» ответы были определены вручную, вы можете найти их в `answers.json`. Я добавил комментарии, объясняющие каждый ответ.

Каждый ответ — это список кортежей, каждый кортеж имеет вид `(correct_answer, score)`. Иногда несколько ответов могут считаться корректными.

Из-за смещения в генераторе вопросов в тестовом прогоне было много вопросов с ответом `N/A`. Это позволило получить высокий счёт, если система просто отвечала `N/A` на все вопросы. Поэтому в раунде 1 мы использовали систему оценивания, похожую на оценивание в Math Kangaroo (международном математическом соревновании).
